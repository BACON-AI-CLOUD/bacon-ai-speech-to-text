# PRD-002: BACON-AI Voice v1.1 - AI Text Refinement

**Version:** 1.1
**Status:** Implemented
**Author:** Orchestrator Agent (Opus 4.6)
**Date:** 2026-02-16
**Owner:** Colin Bacon
**Depends On:** PRD-001 (v1.0 Core STT)

---

## 1. Problem Statement

### 1.1 Current State
BACON-AI Voice v1.0 delivers raw Whisper transcriptions directly to the user. Speech-to-text output inherently contains:
- Filler words (um, uh, ah, hmm, like, you know)
- False starts and self-corrections ("I want to -- no, actually, let me...")
- Run-on sentences without punctuation
- Repetitions and verbal tics
- Rambling structure that doesn't reflect the speaker's actual intent

Users must manually clean up transcriptions before they are useful as AI prompts, code comments, or documentation. This negates much of the speed advantage of voice input.

### 1.2 Desired State
An AI-powered text refinement pipeline that automatically processes raw Whisper output and produces clean, well-structured text while preserving the speaker's exact intent. Users can choose from multiple AI providers (cloud and local) and toggle refinement on or off.

### 1.3 Success Criteria
- SC-101: Refined text removes filler words and false starts while preserving meaning
- SC-102: Refinement adds < 500ms latency to the transcription pipeline
- SC-103: At least one provider works without any API key (Ollama local)
- SC-104: User can see raw vs refined text side-by-side and choose which to use
- SC-105: Refinement works with all three activation modes (push-to-talk, VAD, toggle)
- SC-106: Refined text integrates with keyboard typing (refined text gets typed to target window)
- SC-107: Provider switching works without restarting the backend

---

## 2. Scope

### 2.1 In Scope
| ID | Requirement | Priority |
|----|------------|----------|
| REQ-101 | Refiner pipeline: raw text in, cleaned text out | P0 (Must) |
| REQ-102 | Groq provider (cloud, llama-3.3-70b-versatile, ~200ms) | P0 (Must) |
| REQ-103 | Ollama provider (local, any model, free, no API key) | P0 (Must) |
| REQ-104 | Gemini provider (cloud, Gemini Flash, fast alternative) | P1 (Should) |
| REQ-105 | Refiner on/off toggle in UI | P0 (Must) |
| REQ-106 | Provider selection dropdown in UI | P0 (Must) |
| REQ-107 | API key input fields for cloud providers (Groq, Gemini) | P0 (Must) |
| REQ-108 | Raw vs refined text side-by-side display | P0 (Must) |
| REQ-109 | Customizable cleanup prompt | P1 (Should) |
| REQ-110 | Test refiner with sample text button | P1 (Should) |
| REQ-111 | Auto-refine after each transcription | P0 (Must) |
| REQ-112 | Refined text used for keyboard typing when refiner is on | P0 (Must) |
| REQ-113 | REST endpoint: POST /refiner/process | P0 (Must) |
| REQ-114 | REST endpoint: GET /refiner/config | P0 (Must) |
| REQ-115 | REST endpoint: POST /refiner/test | P1 (Should) |
| REQ-116 | Graceful fallback when provider is unavailable | P0 (Must) |
| REQ-117 | API keys stored in backend .env file only (never in frontend, never logged, never in URLs) | P0 (Must) |
| REQ-118 | PUT /refiner/config endpoint for updating refiner settings | P0 (Must) |
| REQ-119 | Ollama model selection (user picks which local model to use) | P1 (Should) |

### 2.2 Out of Scope
- Translation (refinement is English-to-English cleanup only)
- Summarization (text length should remain similar, not condensed)
- Tone adjustment (formal/casual rewriting)
- Multi-turn conversation context (each refinement is stateless)
- Provider-side prompt tuning UI (advanced prompt engineering)
- Streaming/partial refinement results
- Cost tracking for cloud provider API usage

---

## 3. User Stories

### US-101: Toggle Refiner On/Off
**As a** BACON-AI Voice user
**I want to** toggle the text refiner on or off
**So that** I can choose between raw transcription and AI-cleaned text

**Acceptance Criteria:**
- Toggle switch visible in settings panel (RefinerSettings component)
- When off, transcription flow unchanged from v1.0
- When on, each transcription is automatically sent to the selected AI provider
- Toggle state persists in localStorage across sessions
- Default state: off (opt-in feature)

### US-102: Select AI Provider
**As a** user with different infrastructure preferences
**I want to** choose which AI provider processes my transcriptions
**So that** I can use local (free) or cloud (fast) options

**Acceptance Criteria:**
- Dropdown with three options: Groq, Ollama, Gemini
- Provider selection persists in localStorage
- Switching providers takes effect on the next transcription (no restart needed)
- Provider availability indicated (green/red status dot)
- Default provider: Ollama (works without API key)

### US-103: Enter API Keys
**As a** user who wants to use cloud providers
**I want to** enter my API keys in the settings UI
**So that** the refiner can authenticate with Groq or Gemini

**Acceptance Criteria:**
- Password-type input fields for Groq API key and Gemini API key
- Keys stored in backend .env file only (never in frontend localStorage)
- Keys submitted to backend via PUT /refiner/config and stored in .env file server-side
- GET /refiner/config returns `"configured": true/false` but never the key value
- Show/hide toggle for key visibility during input
- Clear button to remove stored keys (sends empty string via PUT /refiner/config)
- Validation: attempt a test call when key is entered, show success/failure

### US-104: View Raw vs Refined Text
**As a** user who wants to verify refinement quality
**I want to** see the original and refined text side-by-side
**So that** I can confirm the AI preserved my intent

**Acceptance Criteria:**
- TextComparison component shows raw text on the left, refined text on the right
- Differences highlighted (strikethrough on removed filler words, bold on restructured text) - P2 stretch
- User can click to copy either version
- User can select which version gets typed to keyboard
- When refiner is off, only raw text is shown (no comparison view)

### US-105: Customize Cleanup Prompt
**As an** advanced user
**I want to** edit the system prompt used for text refinement
**So that** I can tune the cleanup behavior to my preferences

**Acceptance Criteria:**
- Expandable textarea in RefinerSettings showing current system prompt
- Default prompt provided (removes fillers, fixes grammar, preserves intent)
- Reset button to restore default prompt
- Prompt persists in localStorage
- Changes take effect on next refinement (no restart needed)

### US-106: Test Refiner with Sample Text
**As a** user setting up the refiner
**I want to** test it with sample text before using it live
**So that** I can verify my provider and settings work correctly

**Acceptance Criteria:**
- "Test" button in RefinerSettings
- Pre-filled sample text with common speech artifacts (fillers, repetitions, false starts)
- User can edit the sample text
- Shows raw input and refined output side-by-side
- Displays latency (e.g., "Refined in 187ms")
- Shows error message if provider is unreachable

### US-107: Automatic Refinement After Transcription
**As a** user with the refiner enabled
**I want** transcriptions to be automatically refined
**So that** I get clean text without any extra clicks

**Acceptance Criteria:**
- When refiner is on, each Whisper result is immediately sent to POST /refiner/process
- UI shows "Refining..." state during processing
- If refinement fails, raw text is displayed with an error indicator
- If refinement succeeds, TextComparison shows both versions
- Keyboard typing uses the refined text (when enabled)
- Latency of refinement step is displayed

---

## 4. Technical Requirements

### 4.1 Backend - Refiner Module

#### New Files
```
src/backend/app/refiner/
├── __init__.py
├── refiner.py              # Pipeline: raw text -> AI cleanup -> cleaned text
├── prompts.py              # System prompts for speech cleanup
└── providers/
    ├── __init__.py
    ├── base.py             # Abstract: async def refine(text: str) -> str
    ├── groq_provider.py    # Groq API (llama-3.3-70b-versatile, ~200ms)
    ├── ollama_provider.py  # Local Ollama (any model, http://localhost:11434)
    └── gemini_provider.py  # Google Gemini Flash
```

#### Modified Files
- `src/backend/app/main.py` - Mount refiner API router
- `src/backend/pyproject.toml` - Move `httpx>=0.27` from dev to main dependencies (refiner uses it in production)

#### Technical Specifications
- **TR-101:** Provider base class follows the same `Protocol` pattern as `IntegrationBackend` in `src/backend/app/integrations/base.py`
- **TR-102:** Each provider implements `async def refine(text: str, api_key: str | None, prompt: str) -> RefinerResult`
- **TR-103:** `RefinerResult` dataclass: `refined_text: str`, `provider: str`, `model: str`, `latency_ms: float`, `success: bool`, `error: str | None`
- **TR-104:** Groq provider uses `httpx` async client to call `https://api.groq.com/openai/v1/chat/completions` with model `llama-3.3-70b-versatile`
- **TR-105:** Ollama provider uses `httpx` async client to call `http://localhost:11434/api/generate` (configurable host)
- **TR-106:** Gemini provider uses `httpx` async client to call the Gemini API with model `gemini-2.0-flash`
- **TR-107:** API keys are passed per-request from the frontend, never stored on the backend
- **TR-108:** Default system prompt in `prompts.py` instructs the LLM to: remove filler words, fix grammar/punctuation, remove false starts and repetitions, restructure rambling into clear sentences, preserve the speaker's exact intent
- **TR-109:** Refiner pipeline has a configurable timeout (default: 5 seconds) - falls back to raw text on timeout

#### API Endpoints

**POST /refiner/process**
```json
// Request
{
  "text": "um so I was thinking like we should probably uh refactor the auth module you know",
  "provider": "groq",
  "api_key": "gsk_...",
  "prompt": null  // null = use default prompt
}

// Response (200)
{
  "raw_text": "um so I was thinking like we should probably uh refactor the auth module you know",
  "refined_text": "I was thinking we should refactor the auth module.",
  "provider": "groq",
  "model": "llama-3.3-70b-versatile",
  "latency_ms": 187.4
}

// Response (502 - provider error)
{
  "error": "Groq API returned 401: Invalid API key",
  "raw_text": "um so I was thinking...",
  "refined_text": null,
  "provider": "groq"
}
```

**GET /refiner/config**
```json
// Response (200)
{
  "providers": [
    {
      "name": "groq",
      "display_name": "Groq (Cloud)",
      "model": "llama-3.3-70b-versatile",
      "requires_api_key": true,
      "description": "Fastest cloud inference (~200ms), requires free API key"
    },
    {
      "name": "ollama",
      "display_name": "Ollama (Local)",
      "model": "configurable",
      "requires_api_key": false,
      "description": "Local inference, free, requires Ollama running on localhost:11434"
    },
    {
      "name": "gemini",
      "display_name": "Gemini (Cloud)",
      "model": "gemini-2.0-flash",
      "requires_api_key": true,
      "description": "Google Gemini Flash, fast cloud alternative"
    }
  ],
  "default_prompt": "You are a speech-to-text cleanup assistant..."
}
```

**POST /refiner/test**
```json
// Request
{
  "text": "so um like I was going to say that uh the the thing is we need to uh fix the bug",
  "provider": "groq",
  "api_key": "gsk_..."
}

// Response (200)
{
  "raw_text": "so um like I was going to say that uh the the thing is we need to uh fix the bug",
  "refined_text": "The thing is, we need to fix the bug.",
  "provider": "groq",
  "model": "llama-3.3-70b-versatile",
  "latency_ms": 203.1
}
```

### 4.2 Frontend - Refiner UI

#### New Files
```
src/frontend/src/components/RefinerSettings.tsx   # Provider dropdown, API key inputs, toggle, test button
src/frontend/src/components/RefinerSettings.css
src/frontend/src/components/TextComparison.tsx     # Raw vs refined side-by-side display
src/frontend/src/components/TextComparison.css
```

#### Modified Files
- `src/frontend/src/types/index.ts` - Add `RefinerSettings` type to `AppSettings`
- `src/frontend/src/hooks/useSettings.ts` - Add refiner defaults
- `src/frontend/src/App.tsx` - Wire refiner into transcription flow (after `lastResult`, call `/refiner/process`)
- `src/frontend/src/components/TranscriptionDisplay.tsx` - Integrate `TextComparison` when refiner is active

#### Type Additions (`types/index.ts`)
```typescript
export interface RefinerConfig {
  enabled: boolean;
  provider: 'groq' | 'ollama' | 'gemini';
  customPrompt: string;     // empty = use server default
  // NOTE: API keys are stored backend-side in config.json (REQ-117)
  // Frontend only stores toggle, provider selection, and custom prompt
}

// Add to AppSettings:
// refiner: RefinerConfig;
```

#### Default Settings
```typescript
const DEFAULT_REFINER: RefinerConfig = {
  enabled: false,
  provider: 'ollama',    // Default: Ollama (works without API key)
  customPrompt: '',
};
```

#### Frontend Flow (when refiner enabled)
1. User speaks, Whisper returns raw transcription via WebSocket
2. `App.tsx` receives `lastResult` with raw text
3. If `settings.refiner.enabled`, POST to `/refiner/process` with raw text, provider, API key
4. Display "Refining..." indicator
5. On success, `TextComparison` renders raw vs refined side-by-side
6. If keyboard typing enabled, the refined text is typed (not raw)
7. On failure, display raw text with error badge; user can retry or use raw text

---

## 5. Non-Functional Requirements

| ID | Requirement | Target |
|----|------------|--------|
| NFR-101 | Refiner latency (text in to refined text out) | < 500ms (Groq), < 2s (Ollama), < 500ms (Gemini) |
| NFR-102 | Graceful degradation on provider failure | Fall back to raw text, show error indicator |
| NFR-103 | API key security | Keys in backend .env file only, never in localStorage, never logged, never in URLs |
| NFR-104 | Refiner timeout | 5 seconds max, then fall back to raw text |
| NFR-105 | Provider switching latency | Immediate (no model loading, stateless per-request) |
| NFR-106 | Ollama availability detection | Health check on startup, status indicator in UI |
| NFR-107 | Concurrent refinement requests | Serialize per-user (one at a time, queue or drop) |
| NFR-108 | Text length limit | Max 10,000 characters per refinement request |

---

## 6. Dependencies

| Dependency | Type | Risk | Notes |
|-----------|------|------|-------|
| httpx >= 0.27 | Python library | Low | Async HTTP client for provider API calls |
| Groq API | External service | Low | Free tier available, fast response times |
| Ollama | Local service | Medium | User must install and run Ollama separately |
| Gemini API | External service | Low | Google AI Studio free tier available |
| v1.0 transcription pipeline | Internal | None | Must be working before refiner can process text |

---

## 7. Risks and Mitigations

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Ollama not installed on user's machine | Medium | High | Clear setup instructions, graceful fallback, show installation link |
| Cloud API rate limits (Groq/Gemini) | Low | Medium | Show rate limit errors clearly, suggest Ollama as fallback |
| Refinement changes speaker's intent | High | Low | Default prompt emphasizes intent preservation; side-by-side comparison lets user verify |
| API key exposure in browser | Medium | Low | localStorage only, never in URLs, clear documentation on security model |
| Refiner adds too much latency | Medium | Medium | Timeout at 5s, show latency in UI, user can disable |
| Ollama model too large for user's RAM | Medium | Medium | Recommend small models (llama3.2 3B), document RAM requirements |
| Network issues with cloud providers | Low | Medium | Timeout + fallback to raw text, retry button |

---

## 8. Default Refinement Prompt

```
You are a speech-to-text cleanup assistant. Your job is to take raw speech
transcription and clean it up into clear, well-written text.

Rules:
1. Remove filler words: um, uh, ah, hmm, like, you know, I mean, sort of, kind of
2. Remove false starts and self-corrections: "I want to -- no, actually..."
3. Remove repetitions: "the the", "I think I think"
4. Fix grammar and punctuation
5. Restructure rambling sentences into clear, concise ones
6. PRESERVE the speaker's exact intent and meaning - never add or change ideas
7. Keep technical terms, proper nouns, and domain-specific language exactly as spoken
8. Maintain the speaker's tone (casual, formal, etc.)
9. Output ONLY the cleaned text - no explanations, no quotes, no prefixes

Input: Raw speech transcription
Output: Clean, well-structured text
```

---

## 9. Requirements Traceability Matrix

| Requirement | Feature/Component | User Story | Test Coverage |
|-------------|-------------------|------------|---------------|
| REQ-101 | refiner.py pipeline | US-107 | TUT: Unit test refiner pipeline with mock providers |
| REQ-102 | groq_provider.py | US-102 | TUT: Mock Groq API response, verify cleanup |
| REQ-103 | ollama_provider.py | US-102 | TUT: Mock Ollama API response, verify cleanup |
| REQ-104 | gemini_provider.py | US-102 | TUT: Mock Gemini API response, verify cleanup |
| REQ-105 | RefinerSettings.tsx toggle | US-101 | FUT: Toggle on/off, verify settings persist |
| REQ-106 | RefinerSettings.tsx dropdown | US-102 | FUT: Select provider, verify selection persists |
| REQ-107 | RefinerSettings.tsx API key inputs | US-103 | FUT: Enter key, verify stored in localStorage |
| REQ-108 | TextComparison.tsx | US-104 | FUT: Verify raw and refined text displayed |
| REQ-109 | RefinerSettings.tsx prompt textarea | US-105 | FUT: Edit prompt, verify persists and is sent |
| REQ-110 | RefinerSettings.tsx test button | US-106 | FUT: Click test, verify sample refined |
| REQ-111 | App.tsx auto-refine flow | US-107 | SIT: Full transcription -> refinement flow |
| REQ-112 | TranscriptionDisplay.tsx + keyboard | US-107 | SIT: Verify refined text typed to target window |
| REQ-113 | POST /refiner/process endpoint | US-107 | TUT: API endpoint with mock provider |
| REQ-114 | GET /refiner/config endpoint | US-102 | TUT: API endpoint returns provider list |
| REQ-115 | POST /refiner/test endpoint | US-106 | TUT: API endpoint with sample text |
| REQ-116 | Refiner fallback on error | US-107 | TUT: Provider timeout -> raw text returned |
| REQ-117 | API key security (backend config.json) | US-103 | TUT: Verify keys not in logs; FUT: Verify keys stored backend-side |
| REQ-118 | PUT /refiner/config endpoint | US-102, US-103 | TUT: API endpoint updates config |
| REQ-119 | Ollama model selection | US-102 | FUT: User selects Ollama model from dropdown |

---

## 10. Implementation Phases

### Phase E: AI Text Refinement (v1.1)

**Branch:** `feature/v1.1-text-refinement`

#### Step 1: Backend Refiner Module
- Create `src/backend/app/refiner/` package with provider base class
- Implement Groq, Ollama, and Gemini providers
- Create default cleanup prompt
- Add `/refiner/process`, `/refiner/config`, `/refiner/test` endpoints
- Add `httpx` dependency to `pyproject.toml`
- **Gate:** All three providers respond correctly with mock/real API calls

#### Step 2: Frontend Refiner UI
- Add `RefinerConfig` to `types/index.ts` and `AppSettings`
- Create `RefinerSettings.tsx` component (toggle, provider dropdown, API keys, prompt, test)
- Create `TextComparison.tsx` component (raw vs refined side-by-side)
- Wire into `App.tsx` transcription flow
- Update `TranscriptionDisplay.tsx` to use refined text for keyboard typing
- **Gate:** Toggle refiner, select provider, enter API key, see raw vs refined text

#### Step 3: Integration Testing
- Full flow: speak -> Whisper transcription -> AI refinement -> display -> keyboard typing
- All three providers tested with real API calls
- Fallback behavior verified (provider down -> raw text displayed)
- Settings persistence verified across browser refreshes
- **Gate:** Complete SIT evidence pack with screenshots and API logs

---

*Document end. Reviewed by: TBD (awaiting human checkpoint)*
